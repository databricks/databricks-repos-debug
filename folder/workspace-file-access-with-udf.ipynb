{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa49e41-3373-4b52-8c0c-e691750114fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# sys.path - Repos and files data collection notebook\n",
    "This is a companion notebook for https://go/git-syspath-sop - **you should refer to this document first**. The goal is to collect info and diagose issues with `sys.path` and Workspace file access in and outside of Git folders (repos).\n",
    "\n",
    "The notebook should be run in different \"configurations\" to detect different problems.\n",
    "\n",
    "Depending on the nature of the issue you are trying to debug, you should try running it in different:\n",
    "* locations:\n",
    "    - outside of a Git folder (ie. in a normal Workspace folder)\n",
    "    - in a conventional Git folder (ie. without Git CLI)\n",
    "    - in a Git CLI enabled Git folders (see\n",
    "       [GCP doc](https://docs.databricks.com/gcp/en/repos/git-operations-with-repos#use-git-cli-commands-beta),\n",
    "       [AWS doc](https://docs.databricks.com/aws/en/repos/git-operations-with-repos#use-git-cli-commands-beta),\n",
    "       [Azure doc](https://learn.microsoft.com/en-us/azure/databricks/repos/git-operations-with-repos#use-git-cli))\n",
    "* execution methods:\n",
    "    - interactive - ie. running from the editor\n",
    "    - in a job where it is referenced as a Workspace path\n",
    "        - NOTE: there is a difference between running inside and outsid of a Git folder\n",
    "    - in a \"Git job\" (see [AWS doc](https://docs.databricks.com/aws/en/jobs/configure-job#use-git-with-jobs)) where it is referenced as a file in a repo (conveniently this repo is public)\n",
    "* using different types of compute (see [AWS doc](https://docs.databricks.com/aws/en/compute/)):\n",
    "    - Serverless\n",
    "    - Classic\n",
    "* using different compute configurations:\n",
    "    - different serverless image versions (environments, see [AWS doc](https://docs.databricks.com/aws/en/compute/serverless/dependencies))\n",
    "    - different access modes (see [AWS doc](https://docs.databricks.com/aws/en/compute/use-compute#what-are-compute-access-modes))\n",
    "\n",
    "\n",
    "**Once you run the notebook, follow it for instructions.**\n",
    "\n",
    "Please note that you can inspect the code for more information - hover over the cell and click the show code icon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edc7cd5d-19ba-493a-a589-62aa9a45dbaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Notebook CWD and sys.path configuration\n",
    "\n",
    "The next cell checks the notebook location and prints the assessment of whether this notebook is in a Git folder (repo) or not.\n",
    "\n",
    "> If you just cloned the repo with this notebook, it should obviously state that the notebook is in a Git folder. If you moved it out, expect different outputs.\n",
    "\n",
    "**What should I expect?**\n",
    "\n",
    "The printed information and `CWD` (current working directory) and `sys.path` array ordering should follow [AWS link](https://docs.databricks.com/aws/en/libraries#python-library-precedence).\n",
    "\n",
    "If in a Git folder (repo) `sys.path` array should contain:\n",
    "* the **repo root folder**\n",
    "* and the **notebook parent directory**.\n",
    "Both should be placed close to the begining of `sys.path` array.\n",
    "\n",
    "If not in a Git folder (repo) `sys.path` array should contain:\n",
    "* the **notebook parent directory**.\n",
    "It should be placed close to the enf of `sys.path` array.\n",
    "\n",
    "The printed `CWD` should be equal to the **notebook parent directory**.\n",
    "\n",
    "**What to do if the output seems incorrect?**\n",
    "\n",
    "* **First** - take a look at following cells for known issues.\n",
    "* **Otherwise** - note (copy) the notebook environment and file an ES ticket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4387cca2-c482-47dc-9ef5-73bc3eb9917e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check Notebook Directory and sys path Configuration"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "# Determine if sys.path contains the repo root folder and notebook/file parent directory\n",
    "repo_root = None\n",
    "notebook_dir = os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())\n",
    "if not notebook_dir.startswith(\"/Workspace/\"):\n",
    "    notebook_dir = \"/Workspace\" + (\"\" if notebook_dir.startswith(\"/\") else \"/\") + notebook_dir\n",
    "\n",
    "print(\"Notebook/file parent directory:\")\n",
    "print(\"- value: \", notebook_dir)\n",
    "\n",
    "notebook_path_same_as_cwd = notebook_dir == os.getcwd()\n",
    "print(\"- same as CWD: \", notebook_path_same_as_cwd)\n",
    "\n",
    "notebook_dir_in_path = notebook_dir in sys.path\n",
    "print(\"- in sys.path: \", notebook_dir_in_path)\n",
    "\n",
    "if (notebook_dir and notebook_dir_in_path):\n",
    "  print(\"\\nIn reference to https://docs.databricks.com/aws/en/libraries#python-library-precedence:\")\n",
    "  notebook_pos = sys.path.index(notebook_dir)\n",
    "  next_path = sys.path[notebook_pos + 1] if len(sys.path) > notebook_pos + 1 else None\n",
    "  # Since this notebook does not manipulate sys.path,\n",
    "  # we can assume that if the path is at the end of sys.path, notebook is not in a repo.\n",
    "  is_repo = notebook_pos != (len(sys.path) - 1) and os.path.commonpath([notebook_dir, next_path]) == next_path\n",
    "  if is_repo:\n",
    "      print(textwrap.dedent(f\"\"\"\\\n",
    "             - Notebook/file is likely in a repo:\n",
    "               - sys.path position is low (sys.path[{notebook_pos}])\n",
    "               - next path in sys.path is an ancestor directory (likely repo root): {next_path}\n",
    "            \"\"\"))\n",
    "  else:\n",
    "      print(textwrap.dedent(\n",
    "          f\"\"\"\\\n",
    "          - Notebook/file parent is NOT likely in a repo:\n",
    "            - sys.path position is high (sys.path[{notebook_pos}])\n",
    "            - next path in sys.path is not an ancestor directory (likely not repo root): {next_path}\n",
    "          **If the notebook IS IN A REPO - please file an ES.**\n",
    "          \"\"\"))\n",
    "\n",
    "print(\"\")\n",
    "print(\"---------------------------------\")\n",
    "print(\"sys.path:\\n\", sys.path)\n",
    "print(\"\")\n",
    "print(\"os.getcwd():\\n\", os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79c63943-97d9-43f5-86b8-cc40de742750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verify local file access\n",
    "\n",
    "The next cell checks whether the notebook can access local files (on compute cluster) from Spark worker nodes (via ![UDF](path))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f61f80-3033-4967-aa91-43aebef39c5c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify local file access in Pandas UDF (ie. workers)"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "local_file_content = \"This is a new file with test content.\"\n",
    "\n",
    "@pandas_udf('string')\n",
    "def f0(x):\n",
    "    file_path = \"/tmp/local-file-access-test.txt\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(local_file_content)\n",
    "\n",
    "    file_content = 'not read'\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_content = file.read()        \n",
    "    except FileNotFoundError:\n",
    "        file_content = f'{file_path} not found'\n",
    "    except Exception as e:\n",
    "        file_content = f'An error occurred: {e}'\n",
    "    return pd.Series([file_content])\n",
    "\n",
    "print(\"Verifying access to local files from UDF:\")\n",
    "result = spark.range(1).repartition(1).select(f0(\"id\")).collect()\n",
    "udf_access_result = result[0][0]\n",
    "if udf_access_result == local_file_content:\n",
    "  print(\"- Local files can be accessed from UDF (Workers)\")\n",
    "else:\n",
    "  print(f\"- Error accessing local files from UDF (Workers): {udf_access_result}\")\n",
    "  print(f\"  This might mean that the cluster is misconfigured or there is a bug in runtime.\")\n",
    "  print(f\"  Please verify and contact #help-files if the problem cannot be fixed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5538c04a-b168-49d1-a814-dac1c796e336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verify Workspace file access\n",
    "\n",
    "The next cell checks whether the notebook can access Workspace files from Spark worker nodes (via UDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f14af35c-3697-4c52-978f-2e09fa31e500",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify Workspace file access in Pandas UDF (ie. workers)"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import pandas_udf, lit\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "\n",
    "def print_files_info_on_failure():\n",
    "    print(f\"  Check if Workspace file system (files) is enabled for the Workspace.\")\n",
    "    print(f\"  If files are enabled, please contact #help-files.\")\n",
    "\n",
    "\n",
    "content = \"This is a new file with test content.\"\n",
    "\n",
    "wsfs_file_setup_success = False\n",
    "file_path = None\n",
    "udf_access_result = None\n",
    "\n",
    "def create_test_file():\n",
    "    print(\"Setting up the test file in users home:\")\n",
    "    user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "    base_path = f\"/Workspace/Users/{user}\"\n",
    "\n",
    "    name_suffix = datetime.now().strftime('%Y-%m-%dT%H_%M_%S')\n",
    "    file_path = f\"{base_path}/file-access-test-file-{name_suffix}.txt\"\n",
    "    try:\n",
    "        while os.path.exists(file_path):\n",
    "            name_suffix = ''.join(random.choices(string.ascii_letters + string.digits, k=8))\n",
    "            file_path = f\"{base_path}{name_suffix}.txt\"\n",
    "    except Exception as e:\n",
    "        print(f\"- Cannot access Workspace files to prepare test file: {e}\")\n",
    "        print_files_info_on_failure()\n",
    "        return None, False\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(content)\n",
    "        print(f\"- Successfully set up the test file at {file_path}\")\n",
    "        print(f\"  Driver can write Workspace files\")\n",
    "        return file_path, True\n",
    "    except Exception as e:\n",
    "        print(f\"- Failed to set up the test file: {e}\")\n",
    "        print(f\"  Driver cannot write Workspace files\")\n",
    "        print_files_info_on_failure()\n",
    "        return None, False\n",
    "\n",
    "@pandas_udf('string')\n",
    "def f1(file_path_series: pd.Series) -> pd.Series:\n",
    "    def read_file(file_path_from_series):\n",
    "        try:\n",
    "            with open(file_path_from_series, 'r') as file:\n",
    "                return file.read()\n",
    "        except FileNotFoundError:\n",
    "            return f'{file_path_from_series} not found'\n",
    "        except Exception as e:\n",
    "            return f'An error occurred: {e}'\n",
    "    return file_path_series.apply(read_file)\n",
    "\n",
    "\n",
    "\n",
    "file_path, wsfs_file_setup_success = create_test_file()\n",
    "\n",
    "print(\"\\nVerifying access to Workspace files from UDF:\")\n",
    "if wsfs_file_setup_success and file_path:\n",
    "    result = spark.range(1).repartition(1).select(f1(lit(file_path))).collect()\n",
    "    udf_access_result = result[0][0]\n",
    "    if udf_access_result == content:\n",
    "        print(\"- Workspace files can be accessed from UDF (Workers). Workspace file system seems to work\")\n",
    "    else:\n",
    "        print(f\"- Error accessing Workspace files from UDF (Workers): {udf_access_result}\")\n",
    "        print_files_info_on_failure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "173e309a-c4ec-4d39-961a-e3637a9c1ab6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Notebook Environment Information"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Display notebook environment information\n",
    "print(\"=== Notebook Environment ===\")\n",
    "\n",
    "# Spark version is always available\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "\n",
    "# Try to get Databricks Runtime version from different sources\n",
    "try:\n",
    "    dbr_version = spark.conf.get('spark.databricks.clusterUsageTags.sparkVersion')\n",
    "    print(f\"Databricks Runtime Version: {dbr_version}\")\n",
    "except:\n",
    "    try:\n",
    "        # Alternative way to get DBR version\n",
    "        dbr_version = spark.conf.get('spark.databricks.clusterUsageTags.clusterVersion')\n",
    "        print(f\"Databricks Runtime Version: {dbr_version}\")\n",
    "    except:\n",
    "        print(\"Databricks Runtime Version: Not available (Serverless)\")\n",
    "\n",
    "# Try to get cluster information\n",
    "try:\n",
    "    cluster_id = spark.conf.get('spark.databricks.clusterUsageTags.clusterId')\n",
    "    print(f\"Cluster ID: {cluster_id}\")\n",
    "except:\n",
    "    print(\"Cluster ID: Not available (Serverless)\")\n",
    "\n",
    "try:\n",
    "    cluster_name = spark.conf.get('spark.databricks.clusterUsageTags.clusterName')\n",
    "    print(f\"Cluster Name: {cluster_name}\")\n",
    "except:\n",
    "    print(\"Cluster Name: Not available (Serverless)\")\n",
    "\n",
    "# Cloud provider\n",
    "try:\n",
    "    cloud_provider = spark.conf.get('spark.databricks.cloudProvider')\n",
    "    print(f\"Cloud Provider: {cloud_provider}\")\n",
    "except:\n",
    "    print(\"Cloud Provider: Not available\")\n",
    "\n",
    "# Python version\n",
    "print(f\"Python Version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Get user context\n",
    "user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "print(f\"Current User: {user}\")\n",
    "\n",
    "# Compute type\n",
    "try:\n",
    "    compute_type = spark.conf.get('spark.databricks.clusterSource')\n",
    "    print(f\"Compute Type: {compute_type}\")\n",
    "except:\n",
    "    print(\"Compute Type: Serverless (inferred)\")\n",
    "\n",
    "# Access mode\n",
    "try:\n",
    "    access_mode = spark.conf.get('spark.databricks.clusterUsageTags.dataSecurityMode')\n",
    "    print(f\"Access Mode: {access_mode}\")\n",
    "except:\n",
    "    print(\"Access Mode: Not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a2e3ef4-ce67-41ac-9bb8-e8b31fb788a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The next cell copies the current notebook so it can be conveniently run outside a Git folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a49282eb-38cb-4042-9ab0-470fadeba0de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "print(\"=== Run this notebook outside of a repo ===\")\n",
    "\n",
    "# Suffix to prevent infinite loop\n",
    "COPIED_SUFFIX = \"_copied\"\n",
    "\n",
    "# Get the current notebook path\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "\n",
    "# Prevent infinite loop: if this is the copied notebook, skip copying/running\n",
    "if notebook_path.endswith(COPIED_SUFFIX):\n",
    "    print(\"Detected copied notebook execution. Skipping copy.\")\n",
    "else:\n",
    "    notebook_file_path = notebook_path\n",
    "    if not notebook_file_path.startswith(\"/Workspace/\"):\n",
    "        notebook_file_path = \"/Workspace\" + (\"\" if notebook_file_path.startswith(\"/\") else \"/\") + notebook_file_path\n",
    "    if not notebook_file_path.endswith('.ipynb'):\n",
    "        notebook_file_path += '.ipynb'\n",
    "\n",
    "    # Get the user's home directory\n",
    "    user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "    home_dir = f\"/Workspace/Users/{user}\"\n",
    "\n",
    "    # Add suffix to copied notebook name\n",
    "    base_name = os.path.basename(notebook_path)\n",
    "    copied_name = base_name + COPIED_SUFFIX\n",
    "    dest_path = os.path.join(home_dir, copied_name)\n",
    "    dest_file_path = dest_path + '.ipynb'\n",
    "\n",
    "    print(f\"Copying notebook:\")\n",
    "    print(f\"  from: {notebook_file_path}\")\n",
    "    print(f\"    to: {dest_path}\")\n",
    "    try:\n",
    "        shutil.copy(notebook_file_path, dest_file_path)\n",
    "        print(f\"Navigate to the other notebook to run it and check results: \")\n",
    "        workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "        workspace_id = dbutils.notebook.entry_point.getDbutils().notebook().getContext().workspaceId().get()\n",
    "        notebook_link = f\"{workspace_url}/?o={workspace_id}#workspace/{dest_path}\"\n",
    "        print(f\"  {notebook_link}\")\n",
    "    except Exception as e:\n",
    "        print(f\"- Failed to copy notebook: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "workspace-file-access-with-udf",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
